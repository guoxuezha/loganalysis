verifyUrl: http://123.60.214.137:9090/oauth/verifyToken

business-config:
  authenticationEnable: false
  responseEncryptEnable: false
  AESKey: SKVNMEIHJLOANSKI
  logMonitorEnable: false
  adadDefaultPackage: "defaultPackage"
  logBlockFileRetentionDays: 7
  snmpMonitorEnable: false
  kafkaMainTopic: logRepo3

minio:
  #Minio服务所在地址
  endpoint: http://123.60.214.137:5000/
  #访问的key
  accessKey: Jonny
  #访问的秘钥
  secretKey: F144724684DE8673E7B31C27A234FB76
  #存储桶名称
  bucketName: logrecord

pool:
  poolName: "sop"
  poolType: "M4DBPool"
  dbType: "MYSQL"
  dbVersion: ""
  driverName: "com.mysql.cj.jdbc.Driver"
  jndiName: ""
  jdbcURL: "jdbc:mysql://localhost:3306/loganalysis?useOldAliasMetadataBehavior=true&useUnicode=true&characterEncoding=UTF8&rewriteBatchedStatements=true&useCursorFetch=true&defaultFetchSize=100"
  userName: "root"
  password: "MySqlPassword!!"
  schema: ""
  initialSize: 5
  maxActive: 5
  maxIdle: 5
  minIdle: 0
  maxWait: 2
  increaseStep: 5
  inspectionIntervals: 60
  maxIdleTick: 10
  validationQuery: ""
  fetchSize: 1000
  sshIP: "121.42.24.63"
  sshPort: 22
  sshUserName: "dsm"
  sshPassword: "C6B9D2D0AF389CEFA82927FC21AE1F1B"
  localPort: 18922
  dbIP: "10.128.3.53"
  dbPort: 6079
  sshJdbcURL: "jdbc:mysql://127.0.0.1:18922/jsedc_metadata?useUnicode=true&characterEncoding=UTF8&userCursorFetch=true"
  testOnIdle: false
  passwordEncrypted: false

logging:
  file:
    name: D:/workspace/project/loganalysis/src/main/resources/log/local.log

mybatis-plus:
  # 控制台打印sql语句
  configuration:
    log-impl: org.apache.ibatis.logging.stdout.StdOutImpl

spring:
  datasource:
    dynamic:
      datasource:
        master:
          name: loganalysis
          driver-class-name: com.mysql.cj.jdbc.Driver
          url: jdbc:mysql://localhost:3306/${spring.datasource.dynamic.datasource.master.name}?characterEncoding=utf8&nullCatalogMeansCurrent=true&useSSL=false&useUnicode=true&0=true&useLegacyDatetimeCode=false&serverTimezone=Asia/Shanghai
          username: root
          password: MySqlPassword!!
  kafka:
    ###########【Kafka集群地址】###########
    bootstrap-servers: 123.60.214.137:9092

    ###########【初始化生产者配置】###########
    producer:
      retries: 0
      acks: 1
      # 当生产端积累的消息达到batch-size或接收到消息linger.ms后,生产者就会将消息提交给kafka
      batch-size: 16384
      properties:
        linger:
          ms: 0
          # 自定义分区器
          #partitioner:
          #class: com.felix.kafka.producer.CustomizePartitioner
      buffer-memory: 33554432
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer

    ###########【初始化消费者配置】###########
    # 消费端监听的topic不存在时，项目启动会报错(关掉)
    listener:
      missing-topics-fatal: false
      # 设置批量消费
      type: batch

    consumer:
      properties:
        # 默认的消费组ID
        group:
          id: defaultConsumerGroup
        # 消费会话超时时间(超过这个时间consumer没有发送心跳,就会触发rebalance操作)
        session:
          timeout:
            ms: 120000
        # 消费请求超时时间
        request:
          timeout:
            ms: 120000
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      # 是否自动提交offset
      enable-auto-commit: true
      auto-offset-reset: latest
      # 批量消费每次最多消费多少条消息
      max-poll-records: 500

