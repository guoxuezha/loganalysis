spring:
  mvc:
    static-path-pattern: /**
  web:
    resources:
      static-locations: classpath:/META-INF/resources/
  datasource:
    dynamic: # 多数据源配置
      druid: # Druid 【连接池】相关的全局配置
        initial-size: 5 # 初始连接数
        min-idle: 10 # 最小连接池数量
        max-active: 20 # 最大连接池数量
        max-wait: 600000 # 配置获取连接等待超时的时间，单位：毫秒
        time-between-eviction-runs-millis: 60000 # 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位：毫秒
        min-evictable-idle-time-millis: 300000 # 配置一个连接在池中最小生存的时间，单位：毫秒
        max-evictable-idle-time-millis: 900000 # 配置一个连接在池中最大生存的时间，单位：毫秒
        validation-query: SELECT 1 # 配置检测连接是否有效
        test-while-idle: true
        test-on-borrow: false
        test-on-return: false
      primary: master
      datasource:
        master:
          name: loganalysis
          driver-class-name: com.mysql.cj.jdbc.Driver
          url: jdbc:mysql://localhost:3306/${spring.datasource.dynamic.datasource.master.name}?characterEncoding=utf8&nullCatalogMeansCurrent=true&useSSL=false&useUnicode=true&0=true&useLegacyDatetimeCode=false&serverTimezone=Asia/Shanghai
          username: root
          password: MySqlPassword!!
        slave:
          name: loganalysis
          driver-class-name: com.mysql.cj.jdbc.Driver
          url: jdbc:mysql://121.42.24.63:3306/${spring.datasource.dynamic.datasource.slave.name}?characterEncoding=utf8&nullCatalogMeansCurrent=true&useSSL=false&useUnicode=true&0=true&useLegacyDatetimeCode=false&serverTimezone=Asia/Shanghai
          username: gemuser
          password: gemuser123

  kafka:
    ###########【Kafka集群】###########
    bootstrap-servers: 123.249.42.58:9092

    ###########【初始化生产者配置】###########
    producer:
      retries: 0
      acks: 1
      # 当生产端积累的消息达到batch-size或接收到消息linger.ms后,生产者就会将消息提交给kafka
      batch-size: 16384
      properties:
        linger:
          ms: 0
          # 自定义分区器
          #partitioner:
          #class: com.felix.kafka.producer.CustomizePartitioner
      buffer-memory: 33554432
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer

    ###########【初始化消费者配置】###########
    # 消费端监听的topic不存在时，项目启动会报错(关掉)
    listener:
      missing-topics-fatal: false
      # 设置批量消费
      type: batch

    consumer:
      properties:
        # 默认的消费组ID
        group:
          id: defaultConsumerGroup
        # 消费会话超时时间(超过这个时间consumer没有发送心跳,就会触发rebalance操作)
        session:
          timeout:
            ms: 120000
        # 消费请求超时时间
        request:
          timeout:
            ms: 120000
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      # 是否自动提交offset
      #enable-auto-commit: true
      #auto-offset-reset: latest
      # 批量消费每次最多消费多少条消息
      max-poll-records: 500


mybatis-plus:
  mapper-locations: classpath:mapper/*.xml
  # 控制台打印sql语句
#  configuration:
#    log-impl: org.apache.ibatis.logging.stdout.StdOutImpl

logging:
  file:
    name: D:/workspace/project/loganalysis/src/main/resources/log/local.log

minio:
  #Minio服务所在地址
  endpoint: http://123.249.94.31:5000/
  #访问的key
  accessKey: Jonny
  #访问的秘钥
  secretKey: F144724684DE8673E7B31C27A234FB76
  #存储桶名称
  bucketName: logrecord
